# -*- coding: utf-8 -*-
"""Plant_disease_detection-CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OGH_eOZNLgmg0L3zuJKlKXpyV66taLYO
"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Set the paths to the 'Diseased' and 'Healthy' folders
diseased_folder ='/content/drive/MyDrive/Plant disease detection  Dataset/Diseased '
healthy_folder ='/content/drive/MyDrive/Plant disease detection  Dataset/Healthy'

# Define the target size for resizing
target_size = (224, 224)

# Load the images and create labels
images = []
labels = []

for img_path in os.listdir(diseased_folder):
    img = cv2.imread(os.path.join(diseased_folder, img_path))
    img = cv2.resize(img, target_size)  # Resize the images to the target size
    img = img / 255.0  # Normalize pixel values
    images.append(img)
    labels.append(1)  # Label 1 for diseased

for img_path in os.listdir(healthy_folder):
    img = cv2.imread(os.path.join(healthy_folder, img_path))
    img = cv2.resize(img, target_size)
    img = img / 255.0
    images.append(img)
    labels.append(0)  # Label 0 for healthy

# Convert to NumPy arrays
images = np.array(images)
labels = to_categorical(labels)  # One-hot encoding

# Split the dataset
X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)
datagen.fit(X_train)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

# Define the input shape
input_shape = (*target_size, 3)

# Create the model
model = Sequential()

# Convolutional layers
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

# Flatten the output of the convolutional layers
model.add(Flatten())

# Dense layers
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))  # Change the final layer to have 2 output neurons and softmax activation

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)

# Train the model
epochs = 50
batch_size = 32

history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),
                    steps_per_epoch=len(X_train) // batch_size,
                    epochs=epochs,
                    validation_data=(X_val, y_val),
                    callbacks=[early_stopping, reduce_lr])

# Evaluate the model on the validation set
val_loss, val_acc = model.evaluate(X_val, y_val)
print(f"Validation accuracy: {val_acc}")