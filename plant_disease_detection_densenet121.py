# -*- coding: utf-8 -*-
"""Plant_disease_detection_DenseNet121.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1odna0AG7KTNvZ4nB-IKOr00ZJ9xCvyld
"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam

# Set the paths to the 'Diseased' and 'Healthy' folders
diseased_folder = '/content/drive/MyDrive/Plant disease detection  Dataset/Diseased '
healthy_folder = '/content/drive/MyDrive/Plant disease detection  Dataset/Healthy'

# Define the target size for resizing
target_size = (224, 224)

# Load the images and create labels
images = []
labels = []

for img_path in os.listdir(diseased_folder):
    img = cv2.imread(os.path.join(diseased_folder, img_path))
    img = cv2.resize(img, target_size)  # Resize the images to the target size
    img = img / 255.0  # Normalize pixel values
    images.append(img)
    labels.append(1)  # Label 1 for diseased

for img_path in os.listdir(healthy_folder):
    img = cv2.imread(os.path.join(healthy_folder, img_path))
    img = cv2.resize(img, target_size)
    img = img / 255.0
    images.append(img)
    labels.append(0)  # Label 0 for healthy

# Convert to NumPy arrays
images = np.array(images)
labels = to_categorical(labels)  # One-hot encoding

# Split the dataset
X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)
datagen.fit(X_train)

# Load the DenseNet121 model with pre-trained weights
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model
base_model.trainable = False

# Add custom layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dense(2, activation='softmax')(x)  # 2 classes: Diseased and Healthy

# Create the new model
model = Model(inputs=base_model.input, outputs=x)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)

# Train the model
epochs = 8
batch_size = 32

history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),
                    steps_per_epoch=len(X_train) // batch_size,
                    epochs=epochs,
                    validation_data=(X_val, y_val),
                    callbacks=[early_stopping, reduce_lr])

# Evaluate the model on the validation set
val_loss, val_acc = model.evaluate(X_val, y_val)
print(f"Validation accuracy: {val_acc}")